% Modified based on Xiaoming Sun's template

\documentclass{article}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{indentfirst}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{chngpage}
\usepackage{soul,color}
\usepackage{graphicx,float,wrapfig}
\usepackage{ifpdf}
%\usepackage{CJKspace}
\usepackage{verbatim}
%\usepackage{ctex}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{url}

\usepackage{natbib}

\usepackage[colorlinks, citecolor=blue]{hyperref}


% In case you need to adjust margins:
% \topmargin=-0.45in      %
% \evensidemargin=0in     %
% \oddsidemargin=0in      %
% \textwidth=6.5in        %
% \textheight=9.0in       %
% \headsep=0.25in         %

% Setup the header and footer
% \pagestyle{fancy}                                                       %
% \chead{\Title}  %
% \rhead{\firstxmark}                                                     %
% \lfoot{\lastxmark}                                                      %
% \cfoot{}                                                                %
% \rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}}                          %
% \renewcommand\headrulewidth{0.4pt}                                      %
% \renewcommand\footrulewidth{0.4pt}                                      %

% 可以自定义一些命令
\newcommand{\Answer}{\ \\\textbf{Answer:} }
\newcommand{\Acknowledgement}[1]{\ \\{\bf Acknowledgement:} #1}
\newcommand{\Reference}[1]{\ \\{\bf Reference:} #1}

%\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 标题部分
\title{\textmd{\bf Artificial Intelligence Project Proposal}\\
Hierarchical Reinforcement Learning for Sparse Rewards
}
\date{}
\author{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\begin{spacing}{1.1}
\maketitle %\thispagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Begin edit from here


\section{Problem}

Sparse reward is a fundamental challenging problem for RL. Hierarchical exploration approaches learns to select subgoals and how to achieve subgoals, which seems helpful for reinforcement learning with sparse rewards. There are several papers show that hierarchical exploration allows more quickly to explore regions far away than basic $\epsilon$-greedy exploration.

Some problems are remained. For example, why hierarchical exploration performs better than $\epsilon$-greedy exploration? Is there any other exploration method performing better?

\section{Existing Works}

( \citeauthor{AI-16}, \citeyear{AI-16} )\cite{AI-16} proposed a 2 layer hierarchical framework for reinforment learning in the environments where the feedbacks are sparce. The higher hierarchy (Meta Controller) is responsible for defining goals for the lower hierarchy (Controller). The lower hierarchy try to realize the goals use RL algorithms. The paper also uses deep learning methods (convolution) to train the controllers to realize their goals.

\cite{AI-17} generalized fedual reinforcement learning framework so that it can be used under more settings. He also proposed a new method (transition policy gradient) to train the Manager model. The manager model (higher hierarchy) use extrinsic rewards as input to generate intrinsic rewards for worker model (lower hierarchy). And dilated LSTM is used when training Manager Model since long term memory is prefered for the Manager Model.

\cite{AI-18} pointed out that the prior works depended too much on specific settings. For example, in \cite{AI-16}, we need to build a custom object detector that provides plausible object candidates manually. So they aim to provide a more general model that can fit in more complex settings. In contrast to previous works that needs to set the goal based on the model, the authors use states as goals directly, such that the lower hierarchy can be trained faster and the model can fit in different settings more easily. The author also establishes the concept of off-policy.

Something may be not so related:

\cite{AI-15}

\cite{AI-13}

\section{Limitation}

The main limitation of these works on HRL are they lack theoretical analysis on why the proposed framework works better than $\epsilon-$greedy RL. They provide a lot of settings under which their HRL methods can outperform other models. But the question they still need to answer is under what specific conditions can we gaurantee that HRL is better than traditional $\epsilon-$greedy RL.

\section{Goal}

We hope to achieve one or more goals below.

\begin{itemize}
\item Design a hierarchical exploration algorithm which performs good under specific environment.
\item Improve hierarchical exploration algorithm mentions in papers.
\item Explain why hierarchical exploration performs better than $\epsilon$-greedy exploration.
\item Design a learning algorithm in environments with sparse feedbacks, which performs better than hierarchical exploration.
\end{itemize}


\bibliographystyle{plain}
\bibliography{refx}

% End edit to here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{spacing}
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
